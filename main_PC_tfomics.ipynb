{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN + PC model\n",
    "Scripts for setting up our RCNN + PC model using tfomics (https://github.com/p-koo/tfomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "import sys\n",
    "import h5py\n",
    "import conutils\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from __future__ import print_function \n",
    "import os, sys\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('../Tensor/tfomics')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, learn\n",
    "\n",
    "# import models\n",
    "from model_zoo import fourthplace_connectomics_model\n",
    "from model_zoo import simple_connectomics_model, simple_connectomics_model2\n",
    "from model_zoo import residual_connectomics_model, residual_connectomics_model2,residual_connectomics_model4\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 553 ms\n"
     ]
    }
   ],
   "source": [
    "# Load data -- from https://www.kaggle.com/c/connectomics/data\n",
    "#\n",
    "filename = '../Tensor/kaggle_connect_data/normal_dataset.hdf5'\n",
    "group_name = ['normal_data']\n",
    "dataset = h5py.File(filename,'r')\n",
    "%time F_1 = np.array(dataset['/'+group_name[0]+'/F_1'])\n",
    "scores_1 = np.array(dataset['/'+group_name[0]+'/scores_1'])\n",
    "F_2 = np.array(dataset['/'+group_name[0]+'/F_2'])\n",
    "scores_2 = np.array(dataset['/'+group_name[0]+'/scores_2'])\n",
    "F_3 = np.array(dataset['/'+group_name[0]+'/F_3'])\n",
    "scores_3 = np.array(dataset['/'+group_name[0]+'/scores_3'])\n",
    "F_4 = np.array(dataset['/'+group_name[0]+'/F_4'])\n",
    "scores_4 = np.array(dataset['/'+group_name[0]+'/scores_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load network positions for removing light scattering effects\n",
    "#\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-1/networkPositions_normal-1.txt'\n",
    "pos_1 = np.loadtxt(pos,delimiter=',')\n",
    "F_1ls = conutils.unscatter(F_1.T,pos_1)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-2/networkPositions_normal-2.txt'\n",
    "pos_2 = np.loadtxt(pos,delimiter=',')\n",
    "F_2ls = conutils.unscatter(F_2.T,pos_2)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-3/networkPositions_normal-3.txt'\n",
    "pos_3 = np.loadtxt(pos,delimiter=',')\n",
    "F_3ls = conutils.unscatter(F_3.T,pos_3)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-4/networkPositions_normal-4.txt'\n",
    "pos_4 = np.loadtxt(pos,delimiter=',')\n",
    "F_4ls = conutils.unscatter(F_4.T,pos_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate partial correlation metrics for all datasets\n",
    "#\n",
    "pred_out_1 = conutils.get_partial_corr_scores(F_1ls)\n",
    "pred_out_2 = conutils.get_partial_corr_scores(F_2ls)\n",
    "pred_out_3 = conutils.get_partial_corr_scores(F_3ls)\n",
    "pred_out_4 = conutils.get_partial_corr_scores(F_4ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F_1ls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e871603799ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Downsample data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mds_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroma_ds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF_1ls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#ds_1 = np.diff(ds_1,axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mds_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroma_ds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF_2ls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#ds_2 = np.diff(ds_2,axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F_1ls' is not defined"
     ]
    }
   ],
   "source": [
    "# Downsample data\n",
    "#\n",
    "ds_1 = conutils.roma_ds(F_1ls)\n",
    "ds_2 = conutils.roma_ds(F_2ls)\n",
    "ds_3 = conutils.roma_ds(F_3ls)\n",
    "ds_valid = conutils.roma_ds(F_4ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z-score data\n",
    "#\n",
    "vs_1 = conutils.standardize_rows(ds_1)\n",
    "vs_2 = conutils.standardize_rows(ds_2)\n",
    "vs_3 = conutils.standardize_rows(ds_3)\n",
    "vs_valid = conutils.standardize_rows(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize partial correlation coefficients\n",
    "#\n",
    "p1 = pred_out_1.copy()\n",
    "p2 = pred_out_2.copy()\n",
    "p3 = pred_out_3.copy()\n",
    "p4 = pred_out_4.copy()\n",
    "\n",
    "p1[p1==0] = np.min(p1[p1!=0])\n",
    "p2[p2==0] = np.min(p2[p2!=0])\n",
    "p3[p3==0] = np.min(p3[p3!=0])\n",
    "p4[p4==0] = np.min(p4[p4!=0])\n",
    "\n",
    "p1 = p1 - np.mean(p1)\n",
    "p1 = p1/np.std(p1)\n",
    "\n",
    "p2 = p2 - np.mean(p2)\n",
    "p2 = p2/np.std(p2)\n",
    "\n",
    "p3 = p3 - np.mean(p3)\n",
    "p3 = p3/np.std(p3)\n",
    "\n",
    "p4 = p4 - np.mean(p4)\n",
    "p4 = p4/np.std(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\TFconnect\\conutils\\utils.py:586: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  fluor_tf = np.empty((num_images, 4, num_samples, 1),dtype='float32')\n",
      "D:\\Dropbox\\TFconnect\\conutils\\utils.py:587: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  label_tf = np.zeros((num_images,2),dtype='float32')\n",
      "D:\\Dropbox\\TFconnect\\conutils\\utils.py:618: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  noncons_samp = (np.random.choice(noncons[0],(100-represent)*num_images/100,replace=False),\n",
      "D:\\Dropbox\\TFconnect\\conutils\\utils.py:619: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  np.random.choice(noncons[1],(100-represent)*num_images/100,replace=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target size of processed traces: 1147744. count var: 1147744\n"
     ]
    }
   ],
   "source": [
    "# Create data structure for model input\n",
    "#\n",
    "dtf, ltf = conutils.pairwise_prep_tuple_partialcorr((vs_1,vs_2,vs_3), \n",
    "                                                    (scores_1,scores_2,scores_3), \n",
    "                                                    (p1, p2, p3),\n",
    "                                                   represent=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate data into training and cross-validation sets\n",
    "#\n",
    "inds = np.random.choice(dtf.shape[0],replace=False,size=dtf.shape[0])\n",
    "dtf = dtf[inds,:,:,:]\n",
    "ltf = ltf[inds]\n",
    "\n",
    "crossval = dtf.shape[0]//4\n",
    "dtf_crossval = dtf[:crossval,:,:,:]\n",
    "ltf_crossval = ltf[:crossval,:]\n",
    "dtf = dtf[crossval:,:,:,:]\n",
    "ltf = ltf[crossval:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = dtf\n",
    "y_train = ltf\n",
    "X_valid = dtf_crossval\n",
    "y_valid = ltf_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Network architecture:\n",
      "----------------------------------------------------------------------------\n",
      "layer1: input\n",
      "(?, 4, 330, 1)\n",
      "layer2: conv1\n",
      "(?, 3, 326, 32)\n",
      "layer3: conv1_batch\n",
      "(?, 3, 326, 32)\n",
      "layer4: conv1_active\n",
      "(?, 3, 326, 32)\n",
      "layer5: conv1_dropout\n",
      "(?, 3, 326, 32)\n",
      "layer6: resid1_1resid\n",
      "(?, 3, 326, 32)\n",
      "layer7: resid1_1resid_norm\n",
      "(?, 3, 326, 32)\n",
      "layer8: resid1_1resid_active\n",
      "(?, 3, 326, 32)\n",
      "layer9: resid1_2resid\n",
      "(?, 3, 326, 32)\n",
      "layer10: resid1_2resid_norm\n",
      "(?, 3, 326, 32)\n",
      "layer11: resid1_resid_sum\n",
      "(?, 3, 326, 32)\n",
      "layer12: resid1_resid\n",
      "(?, 3, 326, 32)\n",
      "layer13: resid1_batch\n",
      "(?, 3, 326, 32)\n",
      "layer14: resid1_dropout\n",
      "(?, 3, 326, 32)\n",
      "layer15: conv2\n",
      "(?, 1, 322, 64)\n",
      "layer16: conv2_batch\n",
      "(?, 1, 322, 64)\n",
      "layer17: conv2_active\n",
      "(?, 1, 322, 64)\n",
      "layer18: conv2_dropout\n",
      "(?, 1, 322, 64)\n",
      "layer19: resid2_1resid\n",
      "(?, 1, 322, 64)\n",
      "layer20: resid2_1resid_norm\n",
      "(?, 1, 322, 64)\n",
      "layer21: resid2_1resid_active\n",
      "(?, 1, 322, 64)\n",
      "layer22: resid2_2resid\n",
      "(?, 1, 322, 64)\n",
      "layer23: resid2_2resid_norm\n",
      "(?, 1, 322, 64)\n",
      "layer24: resid2_resid_sum\n",
      "(?, 1, 322, 64)\n",
      "layer25: resid2_resid\n",
      "(?, 1, 322, 64)\n",
      "layer26: resid2_batch\n",
      "(?, 1, 322, 64)\n",
      "layer27: resid2_pool\n",
      "(?, 1, 32, 64)\n",
      "layer28: resid2_dropout\n",
      "(?, 1, 32, 64)\n",
      "layer29: conv3\n",
      "(?, 1, 32, 128)\n",
      "layer30: conv3_batch\n",
      "(?, 1, 32, 128)\n",
      "layer31: conv3_active\n",
      "(?, 1, 32, 128)\n",
      "layer32: conv3_dropout\n",
      "(?, 1, 32, 128)\n",
      "layer33: dense1\n",
      "(?, 256)\n",
      "layer34: dense1_bias\n",
      "(?, 256)\n",
      "layer35: dense1_active\n",
      "(?, 256)\n",
      "layer36: dense1_dropout\n",
      "(?, 256)\n",
      "layer37: resid3_1resid\n",
      "(?, 256)\n",
      "layer38: resid3_1resid_norm\n",
      "(?, 256)\n",
      "layer39: resid3_1resid_active\n",
      "(?, 256)\n",
      "layer40: resid3_2resid\n",
      "(?, 256)\n",
      "layer41: resid3_2resid_norm\n",
      "(?, 256)\n",
      "layer42: resid3_resid_sum\n",
      "(?, 256)\n",
      "layer43: resid3_resid\n",
      "(?, 256)\n",
      "layer44: resid3_batch\n",
      "(?, 256)\n",
      "layer45: resid3_dropout\n",
      "(?, 256)\n",
      "layer46: dense2\n",
      "(?, 2)\n",
      "layer47: dense2_bias\n",
      "(?, 2)\n",
      "layer48: dense2_active\n",
      "(?, 2)\n",
      "layer49: output\n",
      "(?, 2)\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model4.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "data_path = './'\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "output_name = 'dataset1_residual4'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 100 \n",
      "total time: 434.15183210372925\n",
      "num batches: 8609\n",
      "  valid loss:\t\t0.31012\n",
      "  valid accuracy:\t0.87814+/-0.00000\n",
      "  valid auc-roc:\t0.94269+/-0.00000\n",
      "  valid auc-pr:\t\t0.93975+/-0.00176\n",
      "  saving model to:  D:/TFconnect/results\\tfomics\\trackall/fixed\\testbad_partialcorr_2.ckpt\n",
      "INFO:tensorflow:D:/TFconnect/results\\tfomics\\trackall/fixed\\testbad_partialcorr_2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ad99859b3b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'inputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'targets'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keep_prob_conv'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keep_prob_dense'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is_training'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnntrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Dropbox\\Tensor\\tfomics\\tfomics\\learn.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(nntrainer, data, batch_size, num_epochs, patience, verbose, shuffle)\u001b[0m\n\u001b[0;32m     41\u001b[0m                         \u001b[1;31m# check for early stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                                 \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnntrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dropbox\\Tensor\\tfomics\\tfomics\\neuralnetwork.py\u001b[0m in \u001b[0;36mearly_stopping\u001b[1;34m(self, current_loss, patience)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_loss\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_epoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                                 \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "#\n",
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.8, 'keep_prob_dense': 0.5, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=200, \n",
    "                      patience=20, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dat = vs_valid\n",
    "val_lbl = scores_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Wall time: 30min 44s\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on validation data\n",
    "#\n",
    "pred_lbl =  conutils.valid_eval_tfomics_partialcorr(nntrainer,val_dat,p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get ROC-AUC metric\n",
    "#\n",
    "fpr, tpr, thresholds = roc_curve(np.reshape(val_lbl,(1e6,)), pred_lbl)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission(pred_lbl_valid, pred_lbl_test):\n",
    "    \"\"\"\n",
    "    Takes predicted labels / confidences for connections (number between 1 and 0) for both the kaggle\n",
    "        validation dataset and test dataset. Outputs a competition-ready .csv file for submitting to\n",
    "        leaderboard\n",
    "    \n",
    "    inputs---\n",
    "        pred_lbl_valid: for validation set, 1-D numpy array with (num neurons)*(num neurons) \n",
    "            connection predictions. This is the predicted connectivity matrix stitched together\n",
    "            row-wise (i.e. for 1000 neurons, first 1000 entries are connections from neuron 1 to\n",
    "            neuron x)\n",
    "        pred_lbl_test: same as above, for the test data\n",
    "    \"\"\"\n",
    "    filename = './submission_pcorr_dataset3.csv'\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    # write header\n",
    "    f.write('NET_neuronI_neuronJ,Strength\\n')\n",
    "    \n",
    "    Nn = np.sqrt(len(pred_lbl_valid)).astype('int')\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(Nn):\n",
    "        for j in range(Nn):\n",
    "            f.write('valid_{}_{},{}\\n'.format(i+1,j+1,pred_lbl_valid[cnt]))\n",
    "            cnt += 1\n",
    "    \n",
    "    Nn = np.sqrt(len(pred_lbl_test)).astype('int')\n",
    "    cnt = 0\n",
    "    for i in range(Nn):\n",
    "        for j in range(Nn):\n",
    "            f.write('test_{}_{},{}\\n'.format(i+1,j+1,pred_lbl_test[cnt]))\n",
    "            cnt += 1\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission(pred_lbl_val,pred_lbl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "nntrainer.test_model(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "savefile = 'D:/Dropbox/TFconnect/results/tfomics/pickles/dataset3_partialcorr_endpoint_lowcc.pickle'\n",
    "f = open(savefile, 'wb')\n",
    "cPickle.dump(pred_lbl, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "savefile = 'D:/Dropbox/TFconnect/results/tfomics/dataset7_residual4_partialcorr_crossval.pickle'\n",
    "f = open(savefile, 'wb')\n",
    "cPickle.dump(pred_lbl, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savefile = 'D:/Dropbox/TFconnect/results/tfomics/dataset7_residual4_partialcorr_validation.pickle'\n",
    "f = open(savefile, 'wb')\n",
    "cPickle.dump(pred_lbl_val, f)\n",
    "f.close()\n",
    "\n",
    "savefile = 'D:/Dropbox/TFconnect/results/tfomics/dataset7_residual4_partialcorr_test.pickle'\n",
    "f = open(savefile, 'wb')\n",
    "cPickle.dump(pred_lbl_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for possible common input motif problems and compare their partial correlations to real ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_mat_true = scores_1\n",
    "TP = np.where(conn_mat_true)\n",
    "TN = np.where(conn_mat_true==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "poss_commons = []\n",
    "poss_commons_size = []\n",
    "\n",
    "for i in range(len(TN[0])):\n",
    "    # Find false directed connection A->B \n",
    "    A = TN[0][i]\n",
    "    B = TN[1][i]\n",
    "    \n",
    "    # Are the real connections C->A and C->B?\n",
    "    check = np.intersect1d(TP[0][TP[1]==A],TP[0][TP[1]==B]).size\n",
    "    if check != 0 :\n",
    "        cnt += 1\n",
    "        poss_commons.append((A,B))\n",
    "        poss_commons_size.append(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcT = np.zeros((len(TP[0]),))\n",
    "pcFP = np.zeros((len(poss_commons),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(pcT)):\n",
    "    pcT[i] = pred_out[TP[0][i],TP[1][i]]\n",
    "\n",
    "for i in range(len(pcFP)):\n",
    "    pcFP[i] = pred_out[poss_commons[i][0],poss_commons[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.hist(pcFP,100,normed=1,range=[0.9,1])\n",
    "plt.hist(pcT,100,normed=1,range=[0.9,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vs_H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6703cbad0dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresults_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./results/tfomics/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mval_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvs_H\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m files = ['dataset1_partialcorr_endpoint_continued',\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vs_H' is not defined"
     ]
    }
   ],
   "source": [
    "# Remember need to do 'dataset4_residual5_partialcorr_1d_conv_endpoint' separately because of diff. net structure\n",
    "\n",
    "results_path = './results/tfomics/'\n",
    "\n",
    "val_dat = vs_H\n",
    "\n",
    "files = ['dataset1_partialcorr_endpoint_continued',\n",
    "        'dataset2_1d_version_residual4_partialcorr_best',\n",
    "        'dataset3_1d_version_residual4_partialcorr_best',\n",
    "        'dataset5_residual4_partialcorr_2000batch_49',\n",
    "        'dataset1_residual4_lowrep_partialcorr_best',\n",
    "        'dataset6_residual4_derivnpdiffsignals_partialcorr_best',\n",
    "        'dataset7_residual4_partialcorr_201']\n",
    "\n",
    "for fn in files:\n",
    "    nntrainer.set_best_parameters(os.path.join(results_path,fn+'.ckpt'))\n",
    "    pred_lbl =  conutils.valid_eval_tfomics_partialcorr(nntrainer,val_dat,pH)\n",
    "    \n",
    "\n",
    "    savefile = os.path.join(results_path, fn+'_highcon.pickle')\n",
    "    f = open(savefile, 'wb')\n",
    "    cPickle.dump(pred_lbl, f)\n",
    "    f.close()\n",
    "\n",
    "val_dat = vs_L\n",
    "\n",
    "for fn in files:\n",
    "    nntrainer.set_best_parameters(os.path.join(results_path,fn+'.ckpt'))\n",
    "    pred_lbl =  conutils.valid_eval_tfomics_partialcorr(nntrainer,val_dat,pL)\n",
    "    \n",
    "\n",
    "    savefile = os.path.join(results_path, fn+'_lowcon.pickle')\n",
    "    f = open(savefile, 'wb')\n",
    "    cPickle.dump(pred_lbl, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
