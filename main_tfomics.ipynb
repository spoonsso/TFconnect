{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN model\n",
    "Scripts for setting up our RCNN model using tfomics (https://github.com/p-koo/tfomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "import sys\n",
    "import h5py\n",
    "import conutils\n",
    "\n",
    "from __future__ import print_function \n",
    "import os, sys\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('../Tensor/tfomics')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, learn\n",
    "\n",
    "# import models\n",
    "from model_zoo import fourthplace_connectomics_model\n",
    "from model_zoo import simple_connectomics_model, simple_connectomics_model2\n",
    "from model_zoo import residual_connectomics_model, residual_connectomics_model2\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data -- from https://www.kaggle.com/c/connectomics/data\n",
    "#\n",
    "filename = '../Tensor/kaggle_connect_data/normal_dataset.hdf5'\n",
    "group_name = ['normal_data']\n",
    "dataset = h5py.File(filename,'r')\n",
    "%time F_1 = np.array(dataset['/'+group_name[0]+'/F_1'])\n",
    "scores_1 = np.array(dataset['/'+group_name[0]+'/scores_1'])\n",
    "F_2 = np.array(dataset['/'+group_name[0]+'/F_2'])\n",
    "scores_2 = np.array(dataset['/'+group_name[0]+'/scores_2'])\n",
    "F_3 = np.array(dataset['/'+group_name[0]+'/F_3'])\n",
    "scores_3 = np.array(dataset['/'+group_name[0]+'/scores_3'])\n",
    "F_4 = np.array(dataset['/'+group_name[0]+'/F_4'])\n",
    "scores_4 = np.array(dataset['/'+group_name[0]+'/scores_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load network positions for removing light scattering effects\n",
    "#\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-1/networkPositions_normal-1.txt'\n",
    "pos_1 = np.loadtxt(pos,delimiter=',')\n",
    "F_1ls = conutils.unscatter(F_1.T,pos_1)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-2/networkPositions_normal-2.txt'\n",
    "pos_2 = np.loadtxt(pos,delimiter=',')\n",
    "F_2ls = conutils.unscatter(F_2.T,pos_2)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-3/networkPositions_normal-3.txt'\n",
    "pos_3 = np.loadtxt(pos,delimiter=',')\n",
    "F_3ls = conutils.unscatter(F_3.T,pos_3)\n",
    "\n",
    "pos = 'D:/Dropbox/Tensor/kaggle_connect_data/normal-4/networkPositions_normal-4.txt'\n",
    "pos_4 = np.loadtxt(pos,delimiter=',')\n",
    "F_4ls = conutils.unscatter(F_4.T,pos_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Downsample signals\n",
    "#\n",
    "ds_1 = conutils.roma_ds(F_1ls)\n",
    "ds_2 = conutils.roma_ds(F_2ls)\n",
    "ds_3 = conutils.roma_ds(F_3ls)\n",
    "ds_valid = conutils.roma_ds(F_4ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z-score signals\n",
    "vs_1 = conutils.standardize_rows(ds_1)\n",
    "vs_2 = conutils.standardize_rows(ds_2)\n",
    "vs_3 = conutils.standardize_rows(ds_3)\n",
    "vs_valid = conutils.standardize_rows(ds_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prep the data into 330 sample chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtf, ltf = conutils.pairwise_prep_tuple((vs_1,vs_2,vs_3), (scores_1,scores_2,scores_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we can set up our network layers using tfomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate data into training and cross-validation sets\n",
    "#\n",
    "inds = np.random.choice(dtf.shape[0],replace=False,size=dtf.shape[0])\n",
    "dtf = dtf[inds,:,:,:]\n",
    "ltf = ltf[inds]\n",
    "\n",
    "crossval = dtf.shape[0]//4\n",
    "dtf_crossval = dtf[:crossval,:,:,:]\n",
    "ltf_crossval = ltf[:crossval,:]\n",
    "dtf = dtf[crossval:,:,:,:]\n",
    "ltf = ltf[crossval:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = dtf\n",
    "y_train = ltf\n",
    "X_valid = dtf_crossval\n",
    "y_valid = ltf_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "data_path = './'\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "output_name = 'dataset1_residual2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "#\n",
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.8, 'keep_prob_dense': 0.5, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=200, \n",
    "                    patience=20, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dat = vs_valid\n",
    "val_lbl = scores_4\n",
    "true_lbl = np.reshape(scores_4,(1e6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model on validation data\n",
    "#\n",
    "pred_lbl =  conutils.valid_eval_tfomics(nntrainer,val_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get ROC-AUC metric\n",
    "fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "wrk = auc(fpr, tpr)\n",
    "print(wrk)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
